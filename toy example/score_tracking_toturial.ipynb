{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e2b921",
   "metadata": {},
   "source": [
    "We follow the open source codes from https://github.com/NVlabs/edm2 for both dataset construction and diffusion model training.\n",
    "You can directly use the models we trained or retrain the new model following the instruction from the above website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import dnnlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ef63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        phi,                        # Per-component weight: [comp]\n",
    "        mu,                         # Per-component mean: [comp, dim]\n",
    "        Sigma,                      # Per-component covariance matrix: [comp, dim, dim]\n",
    "        sample_lut_size = 64<<10,   # Lookup table size for efficient sampling.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer('phi', torch.tensor(np.asarray(phi) / np.sum(phi), dtype=torch.float32))\n",
    "        self.register_buffer('mu', torch.tensor(np.asarray(mu), dtype=torch.float32))\n",
    "        self.register_buffer('Sigma', torch.tensor(np.asarray(Sigma), dtype=torch.float32))\n",
    "\n",
    "        # Precompute eigendecompositions of Sigma for efficient heat diffusion.\n",
    "        L, Q = torch.linalg.eigh(self.Sigma) # Sigma = Q @ L @ Q\n",
    "        self.register_buffer('_L', L) # L: [comp, dim, dim]\n",
    "        self.register_buffer('_Q', Q) # Q: [comp, dim, dim]\n",
    "\n",
    "        # Precompute lookup table for efficient sampling.\n",
    "        self.register_buffer('_sample_lut', torch.zeros(sample_lut_size, dtype=torch.int64))\n",
    "        phi_ranges = (torch.cat([torch.zeros_like(self.phi[:1]), self.phi.cumsum(0)]) * sample_lut_size + 0.5).to(torch.int32)\n",
    "        for idx, (begin, end) in enumerate(zip(phi_ranges[:-1], phi_ranges[1:])):\n",
    "            self._sample_lut[begin : end] = idx\n",
    "\n",
    "    # Evaluate the terms needed for calculating PDF and score.\n",
    "    def _eval(self, x, sigma=0):                                                    # x: [..., dim], sigma: [...]\n",
    "        L = self._L + sigma[..., None, None] ** 2                                   # L' = L + sigma * I: [..., dim]\n",
    "        d = L.prod(-1)                                                              # d = det(Sigma') = det(Q @ L' @ Q) = det(L'): [...]\n",
    "        y = self.mu - x[..., None, :]                                               # y = mu - x: [..., comp, dim]\n",
    "        z = torch.einsum('...ij,...j,...kj,...k->...i', self._Q, 1 / L, self._Q, y) # z = inv(Sigma') @ (mu - x): [..., comp, dim]\n",
    "        c = self.phi / (((2 * np.pi) ** x.shape[-1]) * d).sqrt()                    # normalization factor of N(x; mu, Sigma')\n",
    "        w = c * (-1/2 * torch.einsum('...i,...i->...', y, z)).exp()                 # w = N(x; mu, Sigma'): [..., comp]\n",
    "        return z, w\n",
    "\n",
    "    # Calculate p(x; sigma) for the given sample points, processing at most the given number of samples at a time.\n",
    "    def pdf(self, x, sigma=0, max_batch_size=1<<14):\n",
    "        sigma = torch.as_tensor(sigma, dtype=torch.float32, device=x.device).broadcast_to(x.shape[:-1])\n",
    "        x_batches = x.flatten(0, -2).split(max_batch_size)\n",
    "        sigma_batches = sigma.flatten().split(max_batch_size)\n",
    "        pdf_batches = [self._eval(xx, ss)[1].sum(-1) for xx, ss in zip(x_batches, sigma_batches)]\n",
    "        return torch.cat(pdf_batches).reshape(x.shape[:-1]) # x.shape[:-1]\n",
    "\n",
    "    # Calculate log(p(x; sigma)) for the given sample points, processing at most the given number of samples at a time.\n",
    "    def logp(self, x, sigma=0, max_batch_size=1<<14):\n",
    "        return self.pdf(x, sigma, max_batch_size).log()\n",
    "\n",
    "    # Calculate \\nabla_x log(p(x; sigma)) for the given sample points.\n",
    "    def score(self, x, sigma=0):\n",
    "        sigma = torch.as_tensor(sigma, dtype=torch.float32, device=x.device).broadcast_to(x.shape[:-1])\n",
    "        z, w = self._eval(x, sigma)\n",
    "        w = w[..., None]\n",
    "        return (w * z).sum(-2) / w.sum(-2) # x.shape\n",
    "\n",
    "    # Draw the given number of random samples from p(x; sigma).\n",
    "    def sample(self, shape, sigma=0, generator=None):\n",
    "        sigma = torch.as_tensor(sigma, dtype=torch.float32, device=self.mu.device).broadcast_to(shape)\n",
    "        i = self._sample_lut[torch.randint(len(self._sample_lut), size=sigma.shape, device=sigma.device, generator=generator)]\n",
    "        L = self._L[i] + sigma[..., None] ** 2                                                  # L' = L + sigma * I: [..., dim]\n",
    "        x = torch.randn(L.shape, device=sigma.device, generator=generator)                      # x ~ N(0, I): [..., dim]\n",
    "        y = torch.einsum('...ij,...j,...kj,...k->...i', self._Q[i], L.sqrt(), self._Q[i], x)    # y = sqrt(Sigma') @ x: [..., dim]\n",
    "        return y + self.mu[i] # [..., dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt(classes='A', device=torch.device('cpu'), seed=2, origin=np.array([0.0030, 0.0325]), scale=np.array([1.3136, 1.3844])):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    comps = []\n",
    "\n",
    "    # Recursive function to generate a given branch of the distribution.\n",
    "    def recurse(cls, depth, pos, angle):\n",
    "        if depth >= 7:\n",
    "            return\n",
    "\n",
    "        # Choose parameters for the current branch.\n",
    "        dir = np.array([np.cos(angle), np.sin(angle)])\n",
    "        dist = 0.292 * (0.8 ** depth) * (rnd.randn() * 0.2 + 1)\n",
    "        thick = 0.2 * (0.8 ** depth) / dist\n",
    "        size = scale * dist * 0.06\n",
    "\n",
    "        # Represent the current branch as a sequence of Gaussian components.\n",
    "        for t in np.linspace(0.07, 0.93, num=8):\n",
    "            c = dnnlib.EasyDict()\n",
    "            c.cls = cls\n",
    "            c.phi = dist * (0.5 ** depth)\n",
    "            c.mu = (pos + dir * dist * t) * scale\n",
    "            c.Sigma = (np.outer(dir, dir) + (np.eye(2) - np.outer(dir, dir)) * (thick ** 2)) * np.outer(size, size)\n",
    "            comps.append(c)\n",
    "\n",
    "        # Generate each child branch.\n",
    "        for sign in [1, -1]:\n",
    "            recurse(cls=cls, depth=(depth + 1), pos=(pos + dir * dist), angle=(angle + sign * (0.7 ** depth) * (rnd.randn() * 0.2 + 1)))\n",
    "\n",
    "    # Generate each class.\n",
    "    recurse(cls='A', depth=0, pos=origin, angle=(np.pi * 0.25))\n",
    "    recurse(cls='B', depth=0, pos=origin, angle=(np.pi * 1.25))\n",
    "\n",
    "    # Construct a GaussianMixture object for the selected classes.\n",
    "    sel = [c for c in comps if c.cls in classes]\n",
    "    distrib = GaussianMixture([c.phi for c in sel], [c.mu for c in sel], [c.Sigma for c in sel])\n",
    "    return distrib.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e294106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFourierProjection(nn.Module):\n",
    "    def __init__(self, embed_dim, scale=30.):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed \n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self, shape, out_c, block_layer):\n",
    "        super(MyBlock, self).__init__()\n",
    "        self.w1 = nn.Linear(shape, out_c)\n",
    "        self.block_layer = block_layer\n",
    "        for nl in range(2,self.block_layer+1):\n",
    "            setattr(self,\"w_\"+str(nl),nn.Linear(out_c,out_c))\n",
    "        self.activation = lambda x: x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.w1(x)\n",
    "        out = self.activation(out)\n",
    "        for nl in range(2,self.block_layer+1):\n",
    "            out = self.activation(getattr(self,\"w_\"+str(nl))(out))\n",
    "        return out\n",
    "\n",
    "class UNet_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, cond_emb_dim = 32, time_emb_dim=32, scale = 9, block_layer = 1):\n",
    "        super(UNet_MLP, self).__init__()\n",
    "\n",
    "        # Sinusoidal embedding\n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "        # Sinusoidal embedding\n",
    "        self.time_embed = nn.Sequential(GaussianFourierProjection(embed_dim=time_emb_dim),\n",
    "         nn.Linear(time_emb_dim, time_emb_dim))\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.cond_embed = MyBlock(cond_dim,cond_emb_dim,block_layer)\n",
    "        self.cond_embed_dim = cond_emb_dim\n",
    "        \n",
    "        # First half\n",
    "        first_num = 2 ** scale\n",
    "        self.x_embed = MyBlock(input_dim,first_num,block_layer)\n",
    "        \n",
    "        self.te1 = self._make_emb(time_emb_dim, first_num)\n",
    "        self.ce1 = self._make_emb(cond_emb_dim, first_num)\n",
    "        self.b1 = MyBlock(first_num, first_num,block_layer)\n",
    "\n",
    "        \n",
    "\n",
    "        second_num = first_num // 2\n",
    "        self.down1 = MyBlock(first_num,second_num,block_layer)\n",
    "        \n",
    "        self.te2 = self._make_emb(time_emb_dim, second_num)\n",
    "        self.ce2 = self._make_emb(cond_emb_dim , second_num)\n",
    "        self.b2 = MyBlock(second_num,second_num,block_layer)\n",
    "    \n",
    "        \n",
    "        third_num = second_num // 2\n",
    "        self.down2 = MyBlock(second_num,third_num,block_layer)\n",
    "\n",
    "\n",
    "        # Bottleneck\n",
    "        self.te_mid = self._make_emb(time_emb_dim, third_num)\n",
    "        self.ce_mid = self._make_emb(cond_emb_dim, third_num)\n",
    "        self.b_mid = MyBlock(third_num, third_num,block_layer)\n",
    "    \n",
    "\n",
    "        # Second half\n",
    "        self.up1 = MyBlock(third_num, second_num,block_layer)\n",
    "\n",
    "        self.te3 = self._make_emb(time_emb_dim, first_num)\n",
    "        self.ce3 = self._make_emb(cond_emb_dim, first_num)\n",
    "        self.b3 = MyBlock(first_num, second_num,block_layer)\n",
    "\n",
    "        self.up2 = MyBlock(second_num, first_num,block_layer)\n",
    "        self.te4 = self._make_emb(time_emb_dim, first_num * 2)\n",
    "        self.ce4 = self._make_emb(cond_emb_dim, first_num * 2)\n",
    "        self.b4 = MyBlock(first_num * 2, first_num,block_layer)\n",
    "        \n",
    "\n",
    "        self.final = nn.Linear(first_num, input_dim)\n",
    "\n",
    "    def forward(self, x0, ti, xc_inp = None):\n",
    "        t = self.act(self.time_embed(ti))\n",
    "        x = self.x_embed(x0)\n",
    "        if xc_inp is None:\n",
    "            xc = torch.zeros(x.shape[0],self.cond_embed_dim).to(x.device) - 1\n",
    "        else:\n",
    "            xc = self.cond_embed(xc_inp)\n",
    "        \n",
    "        out1 = self.b1(x + self.te1(t) + self.ce1(xc))   # (N, first_num) \n",
    "        out2 = self.b2(self.down1(out1) + self.te2(t) + self.ce2(xc))    # (N, second_num)\n",
    "        out_mid = self.b_mid(self.down2(out2)+ self.te_mid(t) + self.ce_mid(xc))   # (N, third_num)\n",
    "        \n",
    "        out3 = torch.cat((out2, self.up1(out_mid)), dim=1)  # (N, first_num)\n",
    "        out4 = self.b3(out3+ self.te3(t)+ self.ce3(xc))    # (N, second)\n",
    "\n",
    "        out5 = torch.cat((out1, self.up2(out4)), dim=1)  # (N, first_num * 2)\n",
    "        out6 = self.b4(out5+ self.te4(t)+ self.ce4(xc))    # (N, first_num)\n",
    "\n",
    "        out = self.final(out6) # (N, out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_emb(self, dim_in, dim_out):\n",
    "        return nn.Linear(dim_in, dim_out)\n",
    "    \n",
    "def index_iterator(data_len, batch_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(data_len)\n",
    "    else:\n",
    "        indices = np.arange(data_len)\n",
    "    for i in range(0, data_len, batch_size):\n",
    "        yield indices[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, dim=None, eps=1e-4):\n",
    "    if dim is None:\n",
    "        dim = list(range(1, x.ndim))\n",
    "    norm = torch.linalg.vector_norm(x, dim=dim, keepdim=True, dtype=torch.float32)\n",
    "    norm = torch.add(eps, norm, alpha=np.sqrt(norm.numel() / x.numel()))\n",
    "    # norm1 = torch.linalg.vector_norm(x, dim=dim, keepdim=True, dtype=torch.float32)\n",
    "    # norm2 = torch.add(eps, norm1, alpha=np.sqrt(norm1.numel() / x.numel()))\n",
    "    # result = x / norm2.to(x.dtype)\n",
    "    # result = x \n",
    "    return x / norm.to(x.dtype)\n",
    "\n",
    "class MPSiLU(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.silu(x) / 0.596\n",
    "\n",
    "class MPLinear(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.weight.copy_(normalize(self.weight))\n",
    "        w = normalize(self.weight) / np.sqrt(self.weight[0].numel())\n",
    "        return x @ w.t()\n",
    "\n",
    "def std_to_exp(std):\n",
    "    std = np.float64(std)\n",
    "    tmp = std.flatten() ** -2\n",
    "    exp = [np.roots([1, 7, 16 - t, 12 - t]).real.max() for t in tmp]\n",
    "    exp = np.float64(exp).reshape(std.shape)\n",
    "    return exp\n",
    "\n",
    "def power_function_beta(std, t_next, t_delta):\n",
    "    beta = (1 - t_delta / t_next) ** (std_to_exp(std) + 1)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        in_dim      = 2,    # Input dimensionality.\n",
    "        num_layers  = 4,    # Number of hidden layers.\n",
    "        hidden_dim  = 64,   # Number of hidden features.\n",
    "        sigma_data  = 0.5,  # Expected standard deviation of the training data.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sigma_data = sigma_data\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        self.layers.append(MPLinear(in_dim + 2, hidden_dim))\n",
    "        for _layer_idx in range(num_layers):\n",
    "            self.layers.append(MPSiLU())\n",
    "            self.layers.append(MPLinear(hidden_dim, hidden_dim))\n",
    "        self.gain = torch.nn.Parameter(torch.zeros([]))\n",
    "\n",
    "    def forward(self, x, sigma=0):\n",
    "        sigma = torch.as_tensor(sigma, dtype=torch.float32, device=x.device).broadcast_to(x.shape[:-1]).unsqueeze(-1)\n",
    "        x = x / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        y = self.layers(torch.cat([x, sigma.log() / 4, torch.ones_like(sigma)], dim=-1))\n",
    "        z = (y ** 2).mean(-1) * self.gain / sigma.squeeze(-1) - 0.5 * (x ** 2).sum(-1) # preconditioning\n",
    "        return z\n",
    "\n",
    "    def logp(self, x, sigma=0):\n",
    "        return self(x, sigma)\n",
    "\n",
    "    def pdf(self, x, sigma=0):\n",
    "        logp = self.logp(x, sigma=sigma)\n",
    "        pdf = (logp - logp.max()).exp()\n",
    "        return pdf\n",
    "\n",
    "    def score(self, x, sigma=0, graph=False):\n",
    "        x = x.detach().requires_grad_(True)\n",
    "        logp = self.logp(x, sigma=sigma)\n",
    "        score = torch.autograd.grad(outputs=[logp.sum()], inputs=[x], create_graph=graph)[0]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdb2b1",
   "metadata": {},
   "source": [
    "We modified the following function to return both the samples and the corresponding accumulated score differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_with_nv_gap_track(net, x_init, guidance=3, gnet=None,print_flg=False, num_steps=32, sigma_min=0.002, sigma_max=5, rho=7):\n",
    "    # Guided denoiser.\n",
    "    def denoise(x, sigma):\n",
    "        net_score = net.score(x, sigma)\n",
    "        if gnet is not None:\n",
    "            gnet_score = gnet.score(x, sigma)\n",
    "            score = gnet_score.lerp(net_score, guidance)\n",
    "            # print(score.shape)\n",
    "            nv_gap = torch.norm((sigma ** 2)*(net_score-gnet_score),dim=-1)\n",
    "            if print_flg:\n",
    "                print(nv_gap)\n",
    "                time.sleep(0.1)\n",
    "        return x + score * (sigma ** 2), nv_gap\n",
    "\n",
    "    # Time step discretization.\n",
    "    nv_gap = torch.zeros(num_steps,len(x_init)).cuda()\n",
    "    # print(nv_gap.requires_grad)\n",
    "    step_indices = torch.arange(num_steps, dtype=torch.float32, device=x_init.device)\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "\n",
    "    # Main sampling loop.\n",
    "    x_cur = x_init\n",
    "    trajectory = [x_cur]\n",
    "    for i, (t_cur, t_next) in tqdm(enumerate(zip(t_steps[:-1], t_steps[1:]))): # 0, ..., N-1\n",
    "\n",
    "        # Euler step.\n",
    "        inter_d,cur_gap = denoise(x_cur, t_cur)\n",
    "        d_cur = (x_cur - inter_d) / t_cur\n",
    "        x_next = x_cur + (t_next - t_cur) * d_cur\n",
    "        nv_gap[i] = cur_gap\n",
    "        # print(cur_gap)\n",
    "\n",
    "        # Apply 2nd order correction.\n",
    "        if i < num_steps - 1:\n",
    "            inter_d,prime_gap = denoise(x_next, t_next) \n",
    "            d_prime = (x_next - inter_d) / t_next\n",
    "            x_next = x_cur + (t_next - t_cur) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "            nv_gap[i] = nv_gap[i] + prime_gap  #second order correction也算上nv gap了，直接加\n",
    "            # print(prime_gap)\n",
    "\n",
    "        # Record trajectory.\n",
    "        x_cur = x_next\n",
    "        trajectory.append(x_cur)\n",
    "    return x_cur,nv_gap.t()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038890e",
   "metadata": {},
   "source": [
    "We plot the color-coded samples with the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1493de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples,nv_gap=None,size=5,color_system = 1,view_x=0.30, view_y=0.30, view_size=1.2, grid_resolution=400,device=torch.device('cuda')):\n",
    "    gridx = torch.linspace(view_x - view_size, view_x + view_size, steps=grid_resolution, device=device)\n",
    "    gridy = torch.linspace(view_y - view_size, view_y + view_size, steps=grid_resolution, device=device)\n",
    "    gridxy = torch.stack(torch.meshgrid(gridx, gridy, indexing='xy'), axis=-1)\n",
    "    plt.xlim(float(gridx[0]), float(gridx[-1]))\n",
    "    plt.ylim(float(gridy[0]), float(gridy[-1]))\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.gca().set_axis_off()\n",
    "\n",
    "    def contours(values, levels, colors=None, cmap=None, alpha=1, linecolors='black', linealpha=1, linewidth=2.5):\n",
    "        values = -(values.max() - values).sqrt().cpu().numpy()\n",
    "        plt.contourf(gridx.cpu().numpy(), gridy.cpu().numpy(), values, levels=levels, antialiased=True, extend='max', colors=colors, cmap=cmap, alpha=alpha)\n",
    "        plt.contour(gridx.cpu().numpy(), gridy.cpu().numpy(), values, levels=levels, antialiased=True, colors=linecolors, alpha=linealpha, linestyles='solid', linewidths=linewidth) \n",
    "    def points(pos, color='black', alpha=1, size=30):\n",
    "        plt.plot(*pos.cpu().numpy().T, '.', markerfacecolor='black', markeredgecolor='none', color=color, alpha=alpha, markersize=size)\n",
    "\n",
    "    contours(gt('AB', device).logp(gridxy), levels=[-2.12, 0], colors=[[0.9,0.9,0.9]], linecolors=[[0.7,0.7,0.7]], linewidth=1.5)\n",
    "    contours(gt('A', device).logp(gridxy), levels=[-2.12, 0], colors=[[1.0,0.8,0.6]], linecolors=[[0.8,0.6,0.5]], linewidth=1.5)\n",
    "    if nv_gap==None:\n",
    "        points(samples, size=6, alpha=0.2)\n",
    "    else:\n",
    "        if color_system ==1:\n",
    "            plt.scatter(samples[:,0].cpu(), samples[:,1].cpu(), c=nv_gap.cpu(), cmap=plt.cm.tab10,s=size,alpha=1)\n",
    "        else:\n",
    "            plt.scatter(samples[:,0].cpu(), samples[:,1].cpu(), c=nv_gap.cpu(), cmap=plt.cm.hot,s=size,alpha=1)\n",
    "        cbar = plt.colorbar() \n",
    "        cbar.ax.tick_params(labelsize=21)\n",
    "        # cbar.pad = 0.001\n",
    "        # plt.subplots_adjust(right=1.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fcbb3",
   "metadata": {},
   "source": [
    "Conduct the sampling and plot the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c44fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 10000\n",
    "device = torch.device('cuda')\n",
    "x_init = gt('A', device).sample(data_size, sigma=5, generator=torch.Generator(device).manual_seed(1))\n",
    "\n",
    "model_good = ToyModel(num_layers=4,hidden_dim=64).to(device)\n",
    "model_good.load_state_dict(torch.load(\"good_version_trial1_4096.pkl\"))\n",
    "model_bad = ToyModel(num_layers=4,hidden_dim=32).to(device)\n",
    "model_bad.load_state_dict(torch.load(\"uncondition_trial3_0512.pkl\"))\n",
    "\n",
    "sample_points_ome2,om2_gap = sampling_with_nv_gap_track(net=model_good, x_init=x_init, guidance=2, gnet=model_bad,print_flg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dee663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_samples(sample_points_ome2,torch.sum(om2_gap,dim=-1),color_system=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1dc39",
   "metadata": {},
   "source": [
    "For a quantitative analysis, we divide ASD into 50 bins and plot the mean log-probability density of samples in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "om2_gap_sum = torch.sum(om2_gap,dim=-1)\n",
    "sorted_sum,sorted_idx = torch.sort(om2_gap_sum)\n",
    "distrib = gt(\"AB\",device)\n",
    "om2_logp = distrib.logp(sample_points_ome2)\n",
    "logp_list = []\n",
    "\n",
    "for i in range(50):\n",
    "    current_idx = sorted_idx[i*200:(i+1)*200]\n",
    "    current_logp = om2_logp[current_idx]\n",
    "    logp_list.append(torch.mean(current_logp).cpu())\n",
    "\n",
    "plt.scatter(np.linspace(0,49,50),logp_list)\n",
    "plt.xlabel(\"accumulated score differences\", fontsize=14)\n",
    "plt.ylabel(\"mean logp\", fontsize=16)\n",
    "plt.tick_params(axis='both', labelsize=12)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
