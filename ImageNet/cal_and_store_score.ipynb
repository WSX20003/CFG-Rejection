{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Calculate and store the ratings of the images in the corresponding folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import os, shutil\n",
    "from torch import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoProcessor, AutoModel\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_PATH = 'D:/Alignment_with_one_sampling/stable_diffusion/CLIPModel/clip-vit-large-patch14'\n",
    "AESTHETIC_PATH = \"D:/Alignment_with_one_sampling/stable_diffusion/Aesthetic_Scorer/sac+logos+ava1-l14-linearMSE.pth\"\n",
    "HPS_V2_PATH = \"D:/Alignment_with_one_sampling/stable_diffusion/HPS_v2_Scorer/HPS_v2_compressed.pt\"\n",
    "PICK_SCORE_PATH = \"D:/Alignment_with_one_sampling/stable_diffusion/PickScore_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pth_files_to_list(directory):\n",
    "    tensor_list = []\n",
    "    \n",
    "    for filename in tqdm.tqdm(os.listdir(directory)):\n",
    "        if filename.endswith(\".pth\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            tensor = torch.load(file_path,weights_only=True)\n",
    "            tensor_list.append(tensor)  \n",
    "             \n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDiff(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(768, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, embed):\n",
    "        return self.layers(embed)\n",
    "\n",
    "class AestheticScorerDiff(torch.nn.Module):\n",
    "    def __init__(self, dtype):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(CLIP_PATH)\n",
    "        self.mlp = MLPDiff()\n",
    "        state_dict = torch.load(AESTHETIC_PATH)\n",
    "        self.mlp.load_state_dict(state_dict)\n",
    "        self.dtype = dtype\n",
    "        self.eval()\n",
    "\n",
    "    def __call__(self, images):\n",
    "        device = next(self.parameters()).device\n",
    "        embed = self.clip.get_image_features(pixel_values=images)\n",
    "        embed = embed / torch.linalg.vector_norm(embed, dim=-1, keepdim=True)\n",
    "        return self.mlp(embed).squeeze(1)\n",
    "\n",
    "def aesthetic_loss_fn(device=None,\n",
    "                     inference_dtype=None):\n",
    "    scorer = AestheticScorerDiff(dtype=inference_dtype).to(device)\n",
    "    scorer.requires_grad_(False)\n",
    "    processor = CLIPProcessor.from_pretrained(CLIP_PATH)\n",
    "    def loss_fn(im_pix_un, prompts=None):\n",
    "        im_pix = processor(images=im_pix_un).to(device)\n",
    "        input_pix = im_pix['pixel_values']\n",
    "        input_pix = torch.tensor(np.stack(input_pix)).to(device)\n",
    "        rewards = scorer(input_pix)\n",
    "        return rewards\n",
    "        \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hps_loss_fn(inference_dtype=None, device=None):\n",
    "    import hpsv2\n",
    "    from hpsv2.src.open_clip import create_model_and_transforms, get_tokenizer\n",
    "\n",
    "    model_name = \"ViT-H-14\"\n",
    "    model, preprocess_train, preprocess_val = create_model_and_transforms(\n",
    "        model_name,\n",
    "        \"D:/Alignment_with_one_sampling/stable_diffusion/CLIPModel/laion2B-s32B-b79K/open_clip_pytorch_model.bin\",\n",
    "        precision=inference_dtype,\n",
    "        device=device,\n",
    "        jit=False,\n",
    "        force_quick_gelu=False,\n",
    "        force_custom_text=False,\n",
    "        force_patch_dropout=False,\n",
    "        force_image_size=None,\n",
    "        pretrained_image=False,\n",
    "        image_mean=None,\n",
    "        image_std=None,\n",
    "        light_augmentation=True,\n",
    "        aug_cfg={},\n",
    "        output_dict=True,\n",
    "        with_score_predictor=False,\n",
    "        with_region_predictor=False\n",
    "    )    \n",
    "    \n",
    "    checkpoint_path = HPS_V2_PATH\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    model = model.to(device, dtype=inference_dtype)\n",
    "    model.eval()\n",
    "        \n",
    "    def loss_fn(img_path, prompts):    \n",
    "        with torch.no_grad():\n",
    "            image = preprocess_val(img_path).unsqueeze(0).to(device=device, non_blocking=True)\n",
    "            text = tokenizer(prompts).to(device=device, non_blocking=True)\n",
    "            outputs = model(image, text)\n",
    "        image_features, text_features = outputs[\"image_features\"], outputs[\"text_features\"]\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        hps_score = torch.diagonal(logits_per_image)\n",
    "        return hps_score\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_loss_fn(inference_dtype=None, device=None):\n",
    "    from open_clip import get_tokenizer\n",
    "\n",
    "    processor_path = \"D:/Alignment_with_one_sampling/stable_diffusion/CLIPModel/laion2B-s32B-b79K\"\n",
    "    processor = AutoProcessor.from_pretrained(processor_path)\n",
    "\n",
    "    model_name = \"ViT-H-14\"\n",
    "    model = AutoModel.from_pretrained(PICK_SCORE_PATH) \n",
    "    \n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    model = model.to(device, dtype=inference_dtype)\n",
    "    model.eval()\n",
    "\n",
    "    def loss_fn(im_pix_un, prompts):    \n",
    "        with torch.no_grad():\n",
    "            caption = tokenizer(prompts)\n",
    "            caption = caption.to(device)\n",
    "\n",
    "            im_pix = processor(images=im_pix_un)\n",
    "        \n",
    "            input_pix = im_pix['pixel_values']\n",
    "            input_pix = torch.tensor(np.stack(input_pix)).to(device)\n",
    "            image_embs = model.get_image_features(input_pix)\n",
    "            image_embs = image_embs / torch.norm(image_embs, dim=-1, keepdim=True)\n",
    "        \n",
    "            text_embs = model.get_text_features(caption)\n",
    "            text_embs = text_embs / torch.norm(text_embs, dim=-1, keepdim=True)\n",
    "            scores = model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n",
    "        return  scores\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_png_files_by_name(file_name, source_dir, target_dir):\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    source_path = os.path.join(source_dir, file_name)\n",
    "    with PIL.Image.open(source_path) as img:\n",
    "        target_path = os.path.join(target_dir, file_name)\n",
    "        img.save(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = ['goldfish','golden retriever','strawberry','giant panda','castle','convertible','reflex camera','viaduct','pomegranate','head cabbage','teddy','sax','parachute','notebook','grand piano','airship','snow leopard','red fox','American lobster','bulbul',\n",
    "                'hummingbird','maillot','wombat','Tibetan mastiff','marmot','beacon','crib','home theater','lipstick','maze','missile','mortarboard','planetarium','scabbard','snowplow','space shuttle','water tower','volcano','school bus','fountain','burrito',\n",
    "                'throne','window shade','military uniform','tusker','valley','yawl','sea anemone','palace','hot pot']\n",
    "paper_idx = [1,207,949,388,483,511,759,888,957,936,850,776,701,681,579,405,289,277,122,16,95,638,106,244,336,437,520,598,629,646,657,667,727,777,803,812,900,980,779,562,965,857,905,652,101,979,914,108,698,926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rest1 in tqdm.tqdm(range(len(paper_idx))):\n",
    "    class_idx = paper_idx[rest1]\n",
    "    nv_directory = 'nv_out_512_CFG_s/class{}'.format(class_idx)\n",
    "    tensor_list = load_pth_files_to_list(nv_directory)\n",
    "    nv_gap_origin = torch.stack(tensor_list)\n",
    "    torch.save(nv_gap_origin,\"concate_nv_gap/512_class{}_2.23_0to10000.pth\".format(class_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pick_score and store it\n",
    "device=torch.device('cuda')\n",
    "pick_score = pick_loss_fn(torch.float32,device)\n",
    "\n",
    "for rest1 in tqdm.tqdm(range(len(paper_idx))):\n",
    "    class_idx = paper_idx[rest1]\n",
    "    input_dir = 'out_512_CFG_s/class{}'.format(class_idx)\n",
    "    cur_prompt = paper_prompt[rest1]\n",
    "    pick_score_list = []\n",
    "    for i in tqdm.tqdm(range(0, 10000)):\n",
    "        file_name = f'{i:06d}.png'\n",
    "        source_path = os.path.join(input_dir, file_name)\n",
    "        image = PIL.Image.open(source_path)\n",
    "        score = pick_score(image,cur_prompt)\n",
    "        pick_score_list.append(score)\n",
    "\n",
    "    cur_score_store = torch.stack(pick_score_list)\n",
    "    torch.save(cur_score_store,\"image_score/pick_score/512_class{}_2.23_0to10000.pth\".format(class_idx))\n",
    "    del cur_score_store\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del pick_score\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate hpv2_score and store it\n",
    "device=torch.device('cuda')\n",
    "hpv2_score = hps_loss_fn(torch.float32,device)\n",
    "\n",
    "for rest1 in tqdm.tqdm(range(len(paper_idx))):\n",
    "    class_idx = paper_idx[rest1]\n",
    "    input_dir = 'out_512_CFG_s/class{}'.format(class_idx)\n",
    "    cur_prompt = paper_prompt[rest1]\n",
    "\n",
    "    file_path = \"image_score/hpv2_score/512_class{}_2.23_0to10000.pth\".format(class_idx)\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"the file exits\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    pick_score_list = []\n",
    "    for i in range(0, 10000):\n",
    "        file_name = f'{i:06d}.png'\n",
    "        source_path = os.path.join(input_dir, file_name)\n",
    "        image = PIL.Image.open(source_path)\n",
    "        score = hpv2_score(image,cur_prompt)\n",
    "        pick_score_list.append(score)\n",
    "\n",
    "    cur_score_store = torch.stack(pick_score_list)\n",
    "    torch.save(cur_score_store,\"image_score/hpv2_score/512_class{}_2.23_0to10000.pth\".format(class_idx))\n",
    "    del cur_score_store\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del hpv2_score\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate aesthetic_score and store it\n",
    "device=torch.device('cuda')\n",
    "aesthetic_score = aesthetic_loss_fn(device,torch.float32)\n",
    "\n",
    "for rest1 in tqdm.tqdm(range(len(paper_idx))):\n",
    "    class_idx = paper_idx[rest1]\n",
    "    input_dir = 'out_512_CFG_s/class{}'.format(class_idx)\n",
    "    cur_prompt = paper_prompt[rest1]\n",
    "    print(cur_prompt)\n",
    "\n",
    "    pick_score_list = []\n",
    "    for i in tqdm.tqdm(range(0, 10000)):\n",
    "        file_name = f'{i:06d}.png'\n",
    "        source_path = os.path.join(input_dir, file_name)\n",
    "        image = PIL.Image.open(source_path)\n",
    "        score = aesthetic_score(image)\n",
    "        pick_score_list.append(score)\n",
    "\n",
    "    cur_score_store = torch.stack(pick_score_list)\n",
    "    print(cur_score_store.shape)\n",
    "    torch.save(cur_score_store,\"image_score/aesthetic_score/512_class{}_2.23_0to10000.pth\".format(class_idx))\n",
    "    print(\"score has been saved as image_score/aesthetic_score/512_class{}_2.23_0to10000.pth\".format(class_idx))\n",
    "    del cur_score_store\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del aesthetic_score\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
