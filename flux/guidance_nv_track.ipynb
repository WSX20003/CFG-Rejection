{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import List, Optional, Union\n",
    "from diffusers import FluxPipeline\n",
    "import numpy as np\n",
    "import time\n",
    "# import tqdm\n",
    "from tqdm import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shift(\n",
    "    image_seq_len,\n",
    "    base_seq_len: int = 256,\n",
    "    max_seq_len: int = 4096,\n",
    "    base_shift: float = 0.5,\n",
    "    max_shift: float = 1.15,\n",
    "):\n",
    "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
    "    b = base_shift - m * base_seq_len\n",
    "    mu = image_seq_len * m + b\n",
    "    return mu\n",
    "\n",
    "\n",
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    sigmas: Optional[List[float]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \n",
    "    if timesteps is not None and sigmas is not None:\n",
    "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    elif sigmas is not None:\n",
    "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accept_sigmas:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_pipe(model,data_type=torch.float16,cpu_flag=True,compile_flag=False,compile_flag_all=False):\n",
    "    if model == 'dev':\n",
    "        model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "        pipe = FluxPipeline.from_pretrained(\"C:/Users/DELL/.cache/huggingface/flux.1-dev-directly-download\", torch_dtype=data_type)\n",
    "    else:\n",
    "        model_id = \"black-forest-labs/FLUX.1-schnell\"\n",
    "        pipe = FluxPipeline.from_pretrained(\"C:/Users/DELL/.cache/huggingface/flux.1-schnell-directly-download\", torch_dtype=data_type)\n",
    "    if cpu_flag:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "    else:\n",
    "        pipe.to(\"cuda\")\n",
    "    if compile_flag:\n",
    "        pipe.transformer = torch.compile(pipe.transformer)\n",
    "    if compile_flag_all:\n",
    "        pipe.vae = torch.compile(pipe.vae)\n",
    "        pipe.text_encoder = torch.compile(pipe.text_encoder)\n",
    "        pipe.text_encoder_2 = torch.compile(pipe.text_encoder_2)\n",
    "\n",
    "    pipe.vae.requires_grad_(False)\n",
    "    pipe.text_encoder_2.requires_grad_(False)\n",
    "    pipe.text_encoder.requires_grad_(False)\n",
    "    pipe.transformer.requires_grad_(False)\n",
    "    pipe.transformer.config.guidance_embeds=True\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(pipe,prompt,num_images):\n",
    "    device = pipe._execution_device\n",
    "    pipe.text_encoder.to(device)\n",
    "    (\n",
    "        prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        text_ids,\n",
    "    ) = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        prompt_2 = None,\n",
    "        device=device,\n",
    "        num_images_per_prompt=num_images,\n",
    "        lora_scale=None,\n",
    "    )   \n",
    "    pipe.text_encoder.cpu()\n",
    "    pipe.text_encoder_2.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(pipe_schnell.text_encoder.device)\n",
    "    return prompt_embeds,pooled_prompt_embeds,text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latents(pipe,num_images,generator,model_dtype):\n",
    "    device = pipe._execution_device\n",
    "    height = pipe.default_sample_size * pipe.vae_scale_factor\n",
    "    width = pipe.default_sample_size * pipe.vae_scale_factor\n",
    "    num_channels_latents = pipe.transformer.config.in_channels // 4\n",
    "    \n",
    "    latents, latent_image_ids = pipe.prepare_latents(\n",
    "        num_images,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        model_dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents=None\n",
    "    )\n",
    "    return latents, latent_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timesteps(pipe, num_inference_steps,latents):\n",
    "    device = pipe._execution_device\n",
    "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n",
    "    image_seq_len = latents.shape[1]\n",
    "    mu = calculate_shift(\n",
    "        image_seq_len,\n",
    "        pipe.scheduler.config.get(\"base_image_seq_len\", 256),\n",
    "        pipe.scheduler.config.get(\"max_image_seq_len\", 4096),\n",
    "        pipe.scheduler.config.get(\"base_shift\", 0.5),\n",
    "        pipe.scheduler.config.get(\"max_shift\", 1.15),\n",
    "    )\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(\n",
    "        pipe.scheduler,\n",
    "        num_inference_steps,\n",
    "        device,\n",
    "        sigmas=sigmas,\n",
    "        mu=mu,\n",
    "    )\n",
    "    # num_warmup_steps = max(len(timesteps) - num_inference_steps * pipe.scheduler.order, 0)\n",
    "    pipe._num_timesteps = len(timesteps)\n",
    "\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def prepare_timesteps_modified(pipe, num_inference_steps,cur_len=4096):\n",
    "    device = pipe._execution_device\n",
    "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n",
    "    image_seq_len = cur_len\n",
    "    mu = calculate_shift(\n",
    "        image_seq_len,\n",
    "        pipe.scheduler.config.get(\"base_image_seq_len\", 256),\n",
    "        pipe.scheduler.config.get(\"max_image_seq_len\", 4096),\n",
    "        pipe.scheduler.config.get(\"base_shift\", 0.5),\n",
    "        pipe.scheduler.config.get(\"max_shift\", 1.15),\n",
    "    )\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(\n",
    "        pipe.scheduler,\n",
    "        num_inference_steps,\n",
    "        device,\n",
    "        sigmas=sigmas,\n",
    "        mu=mu,\n",
    "    )\n",
    "    # num_warmup_steps = max(len(timesteps) - num_inference_steps * pipe.scheduler.order, 0)\n",
    "    pipe._num_timesteps = len(timesteps)\n",
    "\n",
    "    return timesteps, num_inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_guidance(pipe,guidance_scale,latents):\n",
    "    device = pipe._execution_device\n",
    "    if pipe.transformer.config.guidance_embeds:\n",
    "        guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n",
    "        guidance = guidance.expand(latents.shape[0])\n",
    "    else:\n",
    "        guidance = None\n",
    "    return guidance\n",
    "\n",
    "def handle_guidance_modified(pipe,guidance_scale,len_images):\n",
    "    device = pipe._execution_device\n",
    "    if pipe.transformer.config.guidance_embeds:\n",
    "        guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n",
    "        guidance = guidance.expand(len_images)\n",
    "    else:\n",
    "        guidance = None\n",
    "    return guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_noise(pipe,latents,timestep,guidance,pooled_prompt_embeds,prompt_embeds,text_ids,latent_image_ids):\n",
    "    # device = pipe._execution_device\n",
    "    # pipe.transformer.to(device = pipe._execution_device)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = pipe.transformer(\n",
    "            hidden_states=latents,\n",
    "            timestep=timestep / 1000,\n",
    "            guidance=guidance,\n",
    "            pooled_projections=pooled_prompt_embeds,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            txt_ids=text_ids,\n",
    "            img_ids=latent_image_ids,\n",
    "            joint_attention_kwargs={},\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "    return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_noise_with_track(pipe,latents,timestep,guidance1,pooled_prompt_embeds,prompt_embeds,text_ids,latent_image_ids,guidance2=1):\n",
    "    with torch.no_grad():\n",
    "        noise_pred1 = pipe.transformer(\n",
    "            hidden_states=latents,\n",
    "            timestep=timestep / 1000,\n",
    "            guidance=guidance1,\n",
    "            pooled_projections=pooled_prompt_embeds,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            txt_ids=text_ids,\n",
    "            img_ids=latent_image_ids,\n",
    "            joint_attention_kwargs={},\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_pred2 = pipe.transformer(\n",
    "            hidden_states=latents,\n",
    "            timestep=timestep / 1000,\n",
    "            guidance=guidance2,\n",
    "            pooled_projections=pooled_prompt_embeds,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            txt_ids=text_ids,\n",
    "            img_ids=latent_image_ids,\n",
    "            joint_attention_kwargs={},\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "    \n",
    "    noise_gap_origion = (noise_pred1 - noise_pred2)/guidance1[0]\n",
    "    \n",
    "    return noise_pred1,noise_gap_origion.cpu()\n",
    "\n",
    "def pred_noise_with_track_modified(pipe,latents,timestep,guidance1,pooled_prompt_embeds,prompt_embeds,text_ids,latent_image_ids,guidance2=1):\n",
    "    opt_model = torch.compile(pipe.transformer)\n",
    "    with torch.no_grad():\n",
    "        noise_pred1 = opt_model(\n",
    "            hidden_states=latents,\n",
    "            timestep=timestep / 1000,\n",
    "            guidance=guidance1,\n",
    "            pooled_projections=pooled_prompt_embeds,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            txt_ids=text_ids,\n",
    "            img_ids=latent_image_ids,\n",
    "            joint_attention_kwargs={},\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_pred2 = opt_model(\n",
    "            hidden_states=latents,\n",
    "            timestep=timestep / 1000,\n",
    "            guidance=guidance2,\n",
    "            pooled_projections=pooled_prompt_embeds,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            txt_ids=text_ids,\n",
    "            img_ids=latent_image_ids,\n",
    "            joint_attention_kwargs={},\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "    \n",
    "    noise_gap_origion = (noise_pred1 - noise_pred2)/guidance1[0]\n",
    "    \n",
    "    return noise_pred1,noise_gap_origion.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_cur(pipe,noise_pred, t, latents):\n",
    "    latents_dtype = latents.dtype\n",
    "    latents = pipe.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "    if latents.dtype != latents_dtype:\n",
    "        if torch.backends.mps.is_available():\n",
    "            # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "            latents = latents.to(latents_dtype)\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(pipe,latents,offload_flag=True):\n",
    "    pipe._current_timestep = None\n",
    "    height = pipe.default_sample_size * pipe.vae_scale_factor\n",
    "    width = pipe.default_sample_size * pipe.vae_scale_factor\n",
    "\n",
    "    latents = pipe._unpack_latents(latents, height, width, pipe.vae_scale_factor)\n",
    "    latents = (latents / pipe.vae.config.scaling_factor) + pipe.vae.config.shift_factor\n",
    "\n",
    "    output_type=\"pil\"\n",
    "    image = pipe.vae.decode(latents, return_dict=False)[0]\n",
    "    image = pipe.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "    # Offload all models\n",
    "    if offload_flag:\n",
    "        pipe.maybe_free_model_hooks()\n",
    "    else:\n",
    "        pipe.vae.cpu()\n",
    "        pipe.text_encoder_2.cpu()\n",
    "        pipe.text_encoder.cpu()\n",
    "        pipe.transformer.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A city skyline at sunset with clouds forming the words 'Together we rise, apart we fall. Embrace unity!'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"guidance_nv_track_trial2/prompt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guidance_scale = 6\n",
    "guidance_scale = 6\n",
    "cur_path = \"dev-guidance{}\".format(guidance_scale)\n",
    "num_images_per_prompt = 1\n",
    "num_inference_steps= 28\n",
    "\n",
    "\n",
    "model_dtype = torch.float16\n",
    "pipe_schnell = initial_pipe(\"dev\",data_type=model_dtype,cpu_flag=True,compile_flag=True,compile_flag_all=False)\n",
    "nv_gap_store1 = []\n",
    "total_images = 100\n",
    "\n",
    "for num in tqdm(range(total_images)):\n",
    "    prompt = \"A city skyline at sunset with clouds forming the words 'Together we rise, apart we fall. Embrace unity!'\"\n",
    "    prompt_embeds,pooled_prompt_embeds,text_ids = get_embeds(pipe_schnell,prompt,num_images=num_images_per_prompt)\n",
    "    guidance = handle_guidance_modified(pipe_schnell,guidance_scale,num_images_per_prompt)\n",
    "    guidance_2 = handle_guidance_modified(pipe_schnell,1,num_images_per_prompt)\n",
    "\n",
    "    cur_nv_store = []\n",
    "    seed = 42+num\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "    latents, latent_image_ids = prepare_latents(pipe_schnell,num_images_per_prompt,generator,model_dtype)\n",
    "    timesteps, num_inference_steps = prepare_timesteps_modified(pipe_schnell, num_inference_steps,cur_len=4096)\n",
    "\n",
    "    for t in timesteps:\n",
    "        pipe_schnell._current_timestep = t\n",
    "        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n",
    "\n",
    "        noise_pred,nv_gap_origin = pred_noise_with_track_modified(pipe_schnell,latents,timestep,guidance,pooled_prompt_embeds,prompt_embeds,text_ids,latent_image_ids,guidance_2)\n",
    "        latents = denoise_cur(pipe_schnell,noise_pred, t, latents)\n",
    "\n",
    "        cur_nv_store.append(nv_gap_origin.squeeze(0))\n",
    "        del noise_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    images = decode_image(pipe_schnell,latents,offload_flag=False)\n",
    "    image_path = os.path.join(source_dir,cur_path,\"dev-guidance{}-{}.png\".format(guidance_scale,num))\n",
    "    images[0].save(image_path)\n",
    "    nv_gap_store1.append(torch.stack(cur_nv_store))\n",
    "\n",
    "    del latents,prompt_embeds,pooled_prompt_embeds,text_ids,latent_image_ids,timesteps,guidance,guidance_2\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "nv_gap_1 = torch.stack(nv_gap_store1)\n",
    "nv_path = os.path.join(source_dir,cur_path,\"dev-guidance{}.pth\".format(guidance_scale))\n",
    "torch.save(nv_gap_1,nv_path)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux_dev_trial2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
